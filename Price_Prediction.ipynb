{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwuFZp1CfFc5"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import gc\n",
        "import urllib\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "import joblib\n",
        "\n",
        "# Machine Learning and Deep Learning Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
        "from torchvision import models, transforms\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Import Optuna for hyperparameter tuning\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    DATA_PATH = \"/kaggle/input/amazon-ml-final\"\n",
        "    WORK_DIR = \"/kaggle/working/work_dir\"\n",
        "    TRAIN_IMG_DIR = \"./train_images\"\n",
        "    TEST_IMG_DIR = \"./test_images\"\n",
        "    TEXT_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # produces 384-dim embeddings\n",
        "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    IMG_SIZE = 224\n",
        "\n",
        "os.makedirs(Config.WORK_DIR, exist_ok=True)\n",
        "print(f\"Using device: {Config.DEVICE}\")\n",
        "\n",
        "# Load Data\n",
        "def load_data():\n",
        "    \"\"\"Loads train and test CSV files.\"\"\"\n",
        "    train_df = pd.read_csv(os.path.join(Config.DATA_PATH, \"train.csv\"))\n",
        "    test_df = pd.read_csv(os.path.join(Config.DATA_PATH, \"test.csv\"))\n",
        "    print(f\"Train shape: {train_df.shape}\")\n",
        "    print(f\"Test shape: {test_df.shape}\")\n",
        "    print(f\"\\nPrice statistics:\\n{train_df['price'].describe()}\")\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_data()\n",
        "\n",
        "# Image Downloading Utilities\n",
        "train_df[\"image_path\"] = train_df[\"sample_id\"].apply(lambda x: os.path.join(Config.TRAIN_IMG_DIR, f\"{x}.jpg\"))\n",
        "test_df[\"image_path\"] = test_df[\"sample_id\"].apply(lambda x: os.path.join(Config.TEST_IMG_DIR, f\"{x}.jpg\"))\n",
        "\n",
        "# Text Preprocessing & Feature Extraction\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text): return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "class TextFeatureExtractor:\n",
        "    def init(self, model_name=Config.TEXT_MODEL):\n",
        "        print(f\"Loading text model: {model_name}\")\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "        self.model.to(Config.DEVICE)\n",
        "\n",
        "    def extract_features(self, texts, batch_size=32):\n",
        "        texts = [preprocess_text(t) for t in texts]\n",
        "        return self.model.encode(\n",
        "            texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True\n",
        "        )\n",
        "\n",
        "print(\"\\nExtracting text embeddings...\")\n",
        "text_extractor = TextFeatureExtractor()\n",
        "train_text_features = text_extractor.extract_features(train_df[\"catalog_content\"].values)\n",
        "test_text_features = text_extractor.extract_features(test_df[\"catalog_content\"].values)\n",
        "print(f\"Text features shape: {train_text_features.shape}\")\n",
        "del text_extractor; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Compress Features using Autoencoder\n",
        "class AutoEncoder(nn.Module):\n",
        "    def init(self, input_dim, bottleneck_dim=128):\n",
        "        super().init()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512), nn.ReLU(), nn.Linear(512, bottleneck_dim)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(bottleneck_dim, 512), nn.ReLU(), nn.Linear(512, input_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        enc = self.encoder(x)\n",
        "        dec = self.decoder(enc)\n",
        "        return enc, dec\n",
        "\n",
        "def train_autoencoder(features, input_dim, bottleneck_dim=128, epochs=8, batch_size=512):\n",
        "    device = Config.DEVICE\n",
        "    model = AutoEncoder(input_dim, bottleneck_dim).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.MSELoss()\n",
        "    dataset = TensorDataset(torch.tensor(features, dtype=torch.float32))\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    print(\"\\nTraining Autoencoder...\")\n",
        "    model.train()\n",
        "    for ep in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in loader:\n",
        "            x = batch[0].to(device)\n",
        "            _, dec = model(x)\n",
        "            loss = loss_fn(dec, x)\n",
        "            opt.zero_grad(); loss.backward(); opt.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {ep+1}/{epochs} - Loss: {total_loss/len(loader):.6f}\")\n",
        "    return model\n",
        "\n",
        "def apply_autoencoder(model, features, batch_size=512):\n",
        "    device = Config.DEVICE\n",
        "    model.eval()\n",
        "    dataset = TensorDataset(torch.tensor(features, dtype=torch.float32))\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    compressed = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            x = batch[0].to(device)\n",
        "            enc, _ = model(x)\n",
        "            compressed.append(enc.cpu().numpy())\n",
        "    return np.vstack(compressed)\n",
        "\n",
        "embed_dim = train_text_features.shape[1]\n",
        "autoencoder_model = train_autoencoder(train_text_features, input_dim=embed_dim, bottleneck_dim=128)\n",
        "print(\"Applying autoencoder to train and test data...\")\n",
        "train_text_features_small = apply_autoencoder(autoencoder_model, train_text_features)\n",
        "test_text_features_small = apply_autoencoder(autoencoder_model, test_text_features)\n",
        "print(f\"Compressed text shape: {train_text_features_small.shape}\")\n",
        "del train_text_features, test_text_features, autoencoder_model; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Image Feature Extraction\n",
        "class ImageDataset(Dataset):\n",
        "    def init(self, image_paths, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "    def len(self): return len(self.image_paths)\n",
        "    def getitem(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        if not path or not os.path.exists(path):\n",
        "            image = Image.new(\"RGB\", (Config.IMG_SIZE, Config.IMG_SIZE), \"black\")\n",
        "        else:\n",
        "            try: image = Image.open(path).convert(\"RGB\")\n",
        "            except Exception: image = Image.new(\"RGB\", (Config.IMG_SIZE, Config.IMG_SIZE), \"black\")\n",
        "        return self.transform(image) if self.transform else image\n",
        "\n",
        "class ImageFeatureExtractor:\n",
        "    def init(self):\n",
        "        print(\"\\nLoading EfficientNet-B5 backbone...\")\n",
        "        self.model = models.efficientnet_b5(weights='IMAGENET1K_V1')\n",
        "        self.model.classifier = nn.Identity()\n",
        "        self.model.to(Config.DEVICE).eval()\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    def extract_features(self, paths, batch_size=32):\n",
        "        ds = ImageDataset(paths, self.transform)\n",
        "        dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "        feats = []\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(dl, desc=\"Processing images\"):\n",
        "                batch = batch.to(Config.DEVICE)\n",
        "                feats.append(self.model(batch).cpu().numpy())\n",
        "        return np.vstack(feats)\n",
        "\n",
        "image_extractor = ImageFeatureExtractor()\n",
        "train_image_features = image_extractor.extract_features(train_df[\"image_path\"].values)\n",
        "test_image_features = image_extractor.extract_features(test_df[\"image_path\"].values)\n",
        "print(f\"Image features shape: {train_image_features.shape}\")\n",
        "del image_extractor; gc.collect(); torch.cuda.empty_cache()\n",
        "\n",
        "# Combine & Scale\n",
        "print(\"\\nCombining and scaling features...\")\n",
        "train_features = np.concatenate([train_text_features_small, train_image_features], axis=1)\n",
        "test_features = np.concatenate([test_text_features_small, test_image_features], axis=1)\n",
        "print(f\"Combined feature shape: {train_features.shape}\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "train_features_scaled = scaler.fit_transform(train_features)\n",
        "test_features_scaled = scaler.transform(test_features)\n",
        "del train_features, test_features, train_text_features_small, test_text_features_small, train_image_features, test_image_features; gc.collect()\n",
        "\n",
        "# Train/Validation Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    train_features_scaled, train_df[\"price\"].values, test_size=0.1, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# ### MODIFICATION ###: Model Training with Optuna Hyperparameter Tuning\n",
        "def smape(y_true, y_pred):\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "    diff = np.abs(y_true - y_pred) / denom\n",
        "    diff[denom == 0] = 0\n",
        "    return np.mean(diff) * 100\n",
        "\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'objective': 'regression_l1', # MAE\n",
        "        'metric': 'mae',\n",
        "        'n_estimators': 3000,\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True),\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbose': -1\n",
        "    }\n",
        "\n",
        "    model = LGBMRegressor(**params)\n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        eval_metric=\"mae\",\n",
        "        callbacks=[early_stopping(100, verbose=False)]\n",
        "    )\n",
        "\n",
        "    preds = model.predict(X_val)\n",
        "    mae = mean_absolute_error(y_val, preds)\n",
        "    return mae\n",
        "\n",
        "print(\"\\nRunning Optuna to find best hyperparameters...\")\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
        "\n",
        "print(f\"Best trial found: MAE = {study.best_value:.4f}\")\n",
        "print(\"Best hyperparameters:\")\n",
        "for key, value in study.best_params.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nRetraining on full training data with best hyperparameters...\")\n",
        "\n",
        "# Get the best parameters from the Optuna study\n",
        "best_params = study.best_params\n",
        "best_params['random_state'] = 42\n",
        "best_params['n_jobs'] = -1\n",
        "best_params['objective'] = 'regression_l1'\n",
        "best_params['metric'] = 'mae'\n",
        "\n",
        "# First, train a temporary model on the train/val split to find the optimal number of estimators\n",
        "temp_model = LGBMRegressor(**best_params, n_estimators=4000)\n",
        "temp_model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    eval_metric=\"mae\",\n",
        "    callbacks=[early_stopping(100, verbose=False)]\n",
        ")\n",
        "# Use the best iteration from this temporary model for the final model\n",
        "best_iteration = temp_model.best_iteration_ if temp_model.best_iteration_ else 4000\n",
        "print(f\"Optimal number of estimators found: {best_iteration}\")\n",
        "\n",
        "# Now, create and train the final model on the ENTIRE training dataset\n",
        "final_model = LGBMRegressor(**best_params, n_estimators=best_iteration)\n",
        "final_model.fit(train_features_scaled, train_df[\"price\"].values)\n",
        "print(\"✅ Final model training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_preds = np.clip(final_model.predict(test_features_scaled), 0, None)\n",
        "\n",
        "# ### MODIFICATION ###: Save Model, Scaler, and Submission\n",
        "\n",
        "# --- Save the scaler ---\n",
        "scaler_path = os.path.join(Config.WORK_DIR, \"standard_scaler.bin\")\n",
        "joblib.dump(scaler, scaler_path)\n",
        "print(f\"✅ Scaler saved to: {scaler_path}\")\n",
        "\n",
        "# --- Save the LightGBM model ---\n",
        "model_path = os.path.join(Config.WORK_DIR, \"lgbm_final_model.txt\")\n",
        "final_model.save_model(model_path)\n",
        "print(f\"✅ Final model saved to: {model_path}\")\n",
        "\n",
        "# --- Create and save the submission file ---\n",
        "submission = pd.DataFrame({\"sample_id\": test_df[\"sample_id\"], \"price\": test_preds})\n",
        "out_path = os.path.join(Config.WORK_DIR, \"submission1.csv\")\n",
        "submission.to_csv(out_path, index=False)\n",
        "print(f\"\\n✅ Submission file saved: {out_path}\")\n",
        "print(submission.head())"
      ]
    }
  ]
}